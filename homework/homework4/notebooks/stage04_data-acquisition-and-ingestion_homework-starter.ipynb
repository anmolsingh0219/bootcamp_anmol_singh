{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Starter — Stage 04: Data Acquisition and Ingestion\n",
    "Name: Student\n",
    "Date: December 2024\n",
    "\n",
    "## Objectives\n",
    "- API ingestion with secrets in `.env`\n",
    "- Scrape a permitted public table\n",
    "- Validate and save raw data to `data/raw/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALPHAVANTAGE_API_KEY loaded? True\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, datetime as dt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "RAW = pathlib.Path('data/raw'); RAW.mkdir(parents=True, exist_ok=True)\n",
    "load_dotenv(); print('ALPHAVANTAGE_API_KEY loaded?', bool(os.getenv('ALPHAVANTAGE_API_KEY')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers (use or modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts():\n",
    "    return dt.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "def save_csv(df: pd.DataFrame, prefix: str, **meta):\n",
    "    mid = '_'.join([f\"{k}-{v}\" for k,v in meta.items()])\n",
    "    path = RAW / f\"{prefix}_{mid}_{ts()}.csv\"\n",
    "    df.to_csv(path, index=False)\n",
    "    print('Saved', path)\n",
    "    return path\n",
    "\n",
    "def validate(df: pd.DataFrame, required):\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    return {'missing': missing, 'shape': df.shape, 'na_total': int(df.isna().sum().sum())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 — API Pull (Required)\n",
    "Choose an endpoint (e.g., Alpha Vantage or use `yfinance` fallback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker chosen: AAPL\n",
      "Loading API key from .env...\n",
      "Alpha Vantage API key found: True\n",
      "Requesting data from Alpha Vantage API...\n",
      "Converting to DataFrame and parsing dtypes...\n",
      "Alpha Vantage data successfully fetched!\n",
      "\n",
      "Validating data (required columns, NA counts, shape)...\n",
      "Validation results: {'missing': [], 'shape': (100, 2), 'na_total': 0}\n",
      "\n",
      "Sample of API data:\n",
      "        date  adj_close\n",
      "0 2025-08-15     231.59\n",
      "1 2025-08-14     232.78\n",
      "2 2025-08-13     233.33\n",
      "3 2025-08-12     229.65\n",
      "4 2025-08-11     227.18\n",
      "\n",
      "Data types:\n",
      "date         datetime64[ns]\n",
      "adj_close           float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'missing': [], 'shape': (100, 2), 'na_total': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SYMBOL = 'AAPL'\n",
    "print(f\"Ticker chosen: {SYMBOL}\")\n",
    "\n",
    "print(\"Loading API key from .env...\")\n",
    "USE_ALPHA = bool(os.getenv('ALPHAVANTAGE_API_KEY'))\n",
    "print(f\"Alpha Vantage API key found: {USE_ALPHA}\")\n",
    "\n",
    "if USE_ALPHA:\n",
    "    print(\"Requesting data from Alpha Vantage API...\")\n",
    "    url = 'https://www.alphavantage.co/query'\n",
    "    params = {\n",
    "        'function': 'TIME_SERIES_DAILY',\n",
    "        'symbol': SYMBOL,\n",
    "        'apikey': os.getenv('ALPHAVANTAGE_API_KEY')\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        \n",
    "        time_series = data['Time Series (Daily)']\n",
    "        df_api = pd.DataFrame(time_series).T.reset_index()\n",
    "        df_api.columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "        df_api = df_api[['date', 'close']].rename(columns={'close': 'adj_close'})\n",
    "        \n",
    "        print(\"Converting to DataFrame and parsing dtypes...\")\n",
    "        df_api['date'] = pd.to_datetime(df_api['date'])\n",
    "        df_api['adj_close'] = pd.to_numeric(df_api['adj_close'])\n",
    "        print(\"Alpha Vantage data successfully fetched!\")\n",
    "        \n",
    "    except:\n",
    "        print(\"Alpha Vantage failed, using yfinance fallback...\")\n",
    "        USE_ALPHA = False\n",
    "\n",
    "if not USE_ALPHA:\n",
    "    print(\"Requesting data from yfinance...\")\n",
    "    import yfinance as yf\n",
    "    ticker = yf.Ticker(SYMBOL)\n",
    "    df_api = ticker.history(period='3mo').reset_index()[['Date', 'Close']]\n",
    "    df_api.columns = ['date', 'adj_close']\n",
    "    df_api['date'] = pd.to_datetime(df_api['date'])\n",
    "    print(\"yfinance data successfully fetched!\")\n",
    "\n",
    "print(\"\\nValidating data (required columns, NA counts, shape)...\")\n",
    "v_api = validate(df_api, ['date', 'adj_close'])\n",
    "print(f\"Validation results: {v_api}\")\n",
    "\n",
    "print(f\"\\nSample of API data:\")\n",
    "print(df_api.head())\n",
    "print(f\"\\nData types:\")\n",
    "print(df_api.dtypes)\n",
    "\n",
    "v_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving raw CSV to data/raw/...\n",
      "Saved data\\raw\\api_source-alpha_symbol-AAPL_20250817-230500.csv\n",
      "API data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving raw CSV to data/raw/...\")\n",
    "source = 'alpha' if USE_ALPHA else 'yfinance'\n",
    "saved_path = save_csv(df_api.sort_values('date'), prefix='api', source=source, symbol=SYMBOL)\n",
    "print(f\"API data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 — Scrape a Public Table (Required)\n",
    "Replace `SCRAPE_URL` with a permitted page containing a simple table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping S&P 500 companies table...\n",
      "Requesting data from: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\n",
      "Parsing HTML table...\n",
      "Converting to DataFrame...\n",
      "Validating scraped data...\n",
      "Validation results: {'missing': [], 'shape': (10, 8), 'na_total': 0}\n",
      "\n",
      "Sample of scraped data:\n",
      "  Symbol             Security              GICSSector  \\\n",
      "0    MMM                   3M             Industrials   \n",
      "1    AOS          A. O. Smith             Industrials   \n",
      "2    ABT  Abbott Laboratories             Health Care   \n",
      "3   ABBV               AbbVie             Health Care   \n",
      "4    ACN            Accenture  Information Technology   \n",
      "\n",
      "                GICS Sub-Industry    Headquarters Location  Date added  \\\n",
      "0        Industrial Conglomerates    Saint Paul, Minnesota  1957-03-04   \n",
      "1               Building Products     Milwaukee, Wisconsin  2017-07-26   \n",
      "2           Health Care Equipment  North Chicago, Illinois  1957-03-04   \n",
      "3                   Biotechnology  North Chicago, Illinois  2012-12-31   \n",
      "4  IT Consulting & Other Services          Dublin, Ireland  2011-07-06   \n",
      "\n",
      "          CIK      Founded  \n",
      "0  0000066740         1902  \n",
      "1  0000091142         1916  \n",
      "2  0000001800         1888  \n",
      "3  0001551152  2013 (1888)  \n",
      "4  0001467373         1989  \n",
      "\n",
      "Column names:\n",
      "['Symbol', 'Security', 'GICSSector', 'GICS Sub-Industry', 'Headquarters Location', 'Date added', 'CIK', 'Founded']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'missing': [], 'shape': (10, 8), 'na_total': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Scraping S&P 500 companies table...\")\n",
    "SCRAPE_URL = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "\n",
    "print(f\"Requesting data from: {SCRAPE_URL}\")\n",
    "resp = requests.get(SCRAPE_URL, headers=headers, timeout=30)\n",
    "resp.raise_for_status()\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "print(\"Parsing HTML table...\")\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "rows = []\n",
    "for tr in table.find_all('tr'):\n",
    "    row = [cell.get_text(strip=True).split('[')[0].strip() for cell in tr.find_all(['th', 'td'])]\n",
    "    if row:\n",
    "        rows.append(row)\n",
    "\n",
    "print(\"Converting to DataFrame...\")\n",
    "header = rows[0]\n",
    "data = rows[1:11]\n",
    "df_scrape = pd.DataFrame(data, columns=header)\n",
    "df_scrape.columns = [col.replace('\\n', ' ').strip() for col in df_scrape.columns]\n",
    "\n",
    "print(\"Validating scraped data...\")\n",
    "v_scrape = validate(df_scrape, list(df_scrape.columns))\n",
    "print(f\"Validation results: {v_scrape}\")\n",
    "\n",
    "print(f\"\\nSample of scraped data:\")\n",
    "print(df_scrape.head())\n",
    "print(f\"\\nColumn names:\")\n",
    "print(list(df_scrape.columns))\n",
    "\n",
    "v_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving scraped data to CSV...\n",
      "Saved data\\raw\\scrape_site-wikipedia_table-sp500_20250817-230516.csv\n",
      "Scrape data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving scraped data to CSV...\")\n",
    "saved_path = save_csv(df_scrape, prefix='scrape', site='wikipedia', table='sp500')\n",
    "print(\"Scrape data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "### API Source: Alpha Vantage\n",
    "- URL: `https://www.alphavantage.co/query`\n",
    "- Function: `TIME_SERIES_DAILY`\n",
    "- Symbol: `AAPL`\n",
    "- API key loaded from .env file\n",
    "\n",
    "### Scrape Source: Wikipedia S&P 500 Companies\n",
    "- URL: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\n",
    "- Table: First wikitable with company listings\n",
    "- Data: First 10 companies\n",
    "\n",
    "### Assumptions & Risks\n",
    "- API rate limits may apply\n",
    "- Website structure could change\n",
    "- .env file contains API key (not committed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
