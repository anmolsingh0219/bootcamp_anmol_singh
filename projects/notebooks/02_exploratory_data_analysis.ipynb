{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML-Enhanced European Option Pricing: Exploratory Data Analysis\n",
        "\n",
        "This notebook performs comprehensive exploratory data analysis (EDA) on the processed options data to understand:\n",
        "\n",
        "1. **Data Distribution**: Understand the distribution of key variables\n",
        "2. **Black-Scholes Error Analysis**: Examine pricing errors and patterns\n",
        "3. **Feature Relationships**: Analyze correlations and relationships\n",
        "4. **Market Microstructure**: Study bid-ask spreads, volume, and liquidity\n",
        "5. **Outlier Detection**: Identify and analyze outliers\n",
        "6. **Risk Assessment**: Evaluate assumptions and potential risks\n",
        "\n",
        "This analysis will inform our modeling approach and feature selection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Processed Data\n",
        "\n",
        "Load the most recent cleaned options data from the processing pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the most recent processed data\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "# Find the most recent cleaned data file\n",
        "processed_files = glob.glob(f\"{Config.DATA_PROCESSED_PATH}cleaned_options_data_*.csv\")\n",
        "\n",
        "if not processed_files:\n",
        "    print(\"‚ùå No processed data files found!\")\n",
        "    print(f\"Expected location: {Config.DATA_PROCESSED_PATH}\")\n",
        "    print(\"Please run the data fetching and processing notebook first.\")\n",
        "    raise FileNotFoundError(\"No processed data available\")\n",
        "\n",
        "# Get the most recent file\n",
        "latest_file = max(processed_files, key=os.path.getctime)\n",
        "print(f\"üìÇ Loading data from: {latest_file}\")\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(latest_file)\n",
        "df['expiration_date'] = pd.to_datetime(df['expiration_date'])\n",
        "df['fetch_timestamp'] = pd.to_datetime(df['fetch_timestamp'])\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(df)} options contracts\")\n",
        "print(f\"üìä Data shape: {df.shape}\")\n",
        "print(f\"üìÖ Data date range: {df['fetch_timestamp'].min()} to {df['fetch_timestamp'].max()}\")\n",
        "\n",
        "# Display basic info\n",
        "print(f\"\\nüìã Dataset Overview:\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Basic Data Summary and Statistics\n",
        "\n",
        "Examine the basic characteristics and summary statistics of our options dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic data information\n",
        "print(\"üîç Data Types and Missing Values:\")\n",
        "print(\"=\"*50)\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nüìà Summary Statistics:\")\n",
        "print(\"=\"*50)\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\nüìä Key Metrics:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"‚Ä¢ Unique strikes: {df['strike'].nunique()}\")\n",
        "print(f\"‚Ä¢ Strike range: ${df['strike'].min():.2f} - ${df['strike'].max():.2f}\")\n",
        "print(f\"‚Ä¢ Expiration dates: {df['expiration_date'].nunique()}\")\n",
        "print(f\"‚Ä¢ Time to expiry range: {df['time_to_expiry'].min():.4f} - {df['time_to_expiry'].max():.4f} years\")\n",
        "print(f\"‚Ä¢ Implied volatility range: {df['implied_volatility'].min():.2%} - {df['implied_volatility'].max():.2%}\")\n",
        "print(f\"‚Ä¢ Market price range: ${df['market_price'].min():.2f} - ${df['market_price'].max():.2f}\")\n",
        "print(f\"‚Ä¢ Moneyness range: {df['moneyness'].min():.3f} - {df['moneyness'].max():.3f}\")\n",
        "\n",
        "print(\"\\nüìã Missing Values Summary:\")\n",
        "print(\"=\"*50)\n",
        "missing_summary = df.isnull().sum()\n",
        "if missing_summary.sum() > 0:\n",
        "    missing_pct = (missing_summary / len(df) * 100).round(2)\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Missing Count': missing_summary[missing_summary > 0],\n",
        "        'Missing %': missing_pct[missing_summary > 0]\n",
        "    })\n",
        "    print(missing_df)\n",
        "else:\n",
        "    print(\"‚úÖ No missing values found!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Black-Scholes Analysis\n",
        "\n",
        "Calculate theoretical Black-Scholes prices and analyze pricing errors to understand market vs theoretical pricing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Black-Scholes calculator\n",
        "bs_calculator = BlackScholesCalculator()\n",
        "processor = OptionsDataProcessor()\n",
        "\n",
        "print(\"üî¢ Calculating Black-Scholes theoretical prices...\")\n",
        "\n",
        "# Calculate Black-Scholes prices for all options\n",
        "bs_prices = []\n",
        "for _, row in df.iterrows():\n",
        "    try:\n",
        "        bs_price = bs_calculator.calculate_option_price(\n",
        "            S=row['underlying_price'],\n",
        "            K=row['strike'],\n",
        "            T=row['time_to_expiry'],\n",
        "            r=row['risk_free_rate'],\n",
        "            sigma=row['implied_volatility'],\n",
        "            option_type='call'\n",
        "        )\n",
        "        bs_prices.append(bs_price)\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating BS price for row {_}: {e}\")\n",
        "        bs_prices.append(np.nan)\n",
        "\n",
        "# Add Black-Scholes analysis to dataframe\n",
        "df_analysis = processor.calculate_target_variable(df, pd.Series(bs_prices))\n",
        "\n",
        "print(f\"‚úÖ Black-Scholes prices calculated for {len(df_analysis)} options\")\n",
        "print(f\"\\nüìä Pricing Error Analysis:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"‚Ä¢ Mean pricing error: ${df_analysis['pricing_error'].mean():.4f}\")\n",
        "print(f\"‚Ä¢ Median pricing error: ${df_analysis['pricing_error'].median():.4f}\")\n",
        "print(f\"‚Ä¢ Std pricing error: ${df_analysis['pricing_error'].std():.4f}\")\n",
        "print(f\"‚Ä¢ Min pricing error: ${df_analysis['pricing_error'].min():.4f}\")\n",
        "print(f\"‚Ä¢ Max pricing error: ${df_analysis['pricing_error'].max():.4f}\")\n",
        "\n",
        "print(f\"\\nüìä Relative Error Analysis:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"‚Ä¢ Mean relative error: {df_analysis['relative_error'].mean():.2%}\")\n",
        "print(f\"‚Ä¢ Median relative error: {df_analysis['relative_error'].median():.2%}\")\n",
        "print(f\"‚Ä¢ Std relative error: {df_analysis['relative_error'].std():.2%}\")\n",
        "\n",
        "# Show sample with BS prices\n",
        "print(f\"\\nüîç Sample Data with Black-Scholes Prices:\")\n",
        "print(\"=\"*50)\n",
        "sample_cols = ['strike', 'market_price', 'bs_price', 'pricing_error', 'relative_error', 'moneyness', 'implied_volatility']\n",
        "print(df_analysis[sample_cols].head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Distribution Analysis\n",
        "\n",
        "Visualize the distributions of key variables to understand data characteristics and identify patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create distribution plots\n",
        "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
        "fig.suptitle('Distribution Analysis of Key Variables', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Variables to plot\n",
        "variables = [\n",
        "    ('market_price', 'Market Price ($)'),\n",
        "    ('bs_price', 'Black-Scholes Price ($)'),\n",
        "    ('pricing_error', 'Pricing Error ($)'),\n",
        "    ('relative_error', 'Relative Error (%)'),\n",
        "    ('implied_volatility', 'Implied Volatility'),\n",
        "    ('moneyness', 'Moneyness (S/K)'),\n",
        "    ('time_to_expiry', 'Time to Expiry (years)'),\n",
        "    ('volume', 'Volume'),\n",
        "    ('open_interest', 'Open Interest')\n",
        "]\n",
        "\n",
        "for i, (var, title) in enumerate(variables):\n",
        "    row, col = i // 3, i % 3\n",
        "    ax = axes[row, col]\n",
        "    \n",
        "    # Handle missing values for volume\n",
        "    if var == 'volume':\n",
        "        data = df_analysis[var].fillna(0)\n",
        "    elif var == 'relative_error':\n",
        "        data = df_analysis[var] * 100  # Convert to percentage\n",
        "    else:\n",
        "        data = df_analysis[var]\n",
        "    \n",
        "    # Create histogram with KDE\n",
        "    ax.hist(data, bins=20, alpha=0.7, density=True, color='skyblue', edgecolor='black')\n",
        "    \n",
        "    # Add KDE if data is not constant\n",
        "    if data.std() > 0:\n",
        "        from scipy.stats import gaussian_kde\n",
        "        kde = gaussian_kde(data.dropna())\n",
        "        x_range = np.linspace(data.min(), data.max(), 100)\n",
        "        ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')\n",
        "    \n",
        "    ax.set_title(title, fontweight='bold')\n",
        "    ax.set_xlabel(title)\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add statistics text\n",
        "    mean_val = data.mean()\n",
        "    std_val = data.std()\n",
        "    ax.text(0.05, 0.95, f'Mean: {mean_val:.4f}\\\\nStd: {std_val:.4f}', \n",
        "            transform=ax.transAxes, verticalalignment='top', \n",
        "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print distribution statistics\n",
        "print(\"üìä Distribution Statistics Summary:\")\n",
        "print(\"=\"*70)\n",
        "for var, title in variables:\n",
        "    if var == 'volume':\n",
        "        data = df_analysis[var].fillna(0)\n",
        "    else:\n",
        "        data = df_analysis[var]\n",
        "    \n",
        "    print(f\"{title}:\")\n",
        "    print(f\"  Mean: {data.mean():.4f}, Median: {data.median():.4f}, Std: {data.std():.4f}\")\n",
        "    print(f\"  Skewness: {stats.skew(data.dropna()):.4f}, Kurtosis: {stats.kurtosis(data.dropna()):.4f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Correlation Analysis\n",
        "\n",
        "Examine relationships between variables to understand feature dependencies and multicollinearity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select numerical columns for correlation analysis\n",
        "numerical_cols = df_analysis.select_dtypes(include=[np.number]).columns.tolist()\n",
        "# Remove ID-like columns\n",
        "exclude_cols = ['fetch_timestamp']\n",
        "numerical_cols = [col for col in numerical_cols if col not in exclude_cols]\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df_analysis[numerical_cols].corr()\n",
        "\n",
        "# Create correlation heatmap\n",
        "plt.figure(figsize=(16, 12))\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Mask upper triangle\n",
        "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8}, fmt='.3f')\n",
        "plt.title('Correlation Matrix of Options Variables', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find highly correlated pairs\n",
        "print(\"üîç Highly Correlated Variable Pairs (|correlation| > 0.7):\")\n",
        "print(\"=\"*60)\n",
        "high_corr_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr_val = correlation_matrix.iloc[i, j]\n",
        "        if abs(corr_val) > 0.7:\n",
        "            var1 = correlation_matrix.columns[i]\n",
        "            var2 = correlation_matrix.columns[j]\n",
        "            high_corr_pairs.append((var1, var2, corr_val))\n",
        "            print(f\"‚Ä¢ {var1} ‚Üî {var2}: {corr_val:.3f}\")\n",
        "\n",
        "if not high_corr_pairs:\n",
        "    print(\"‚úÖ No highly correlated pairs found (|correlation| > 0.7)\")\n",
        "\n",
        "# Correlation with target variable (pricing_error)\n",
        "if 'pricing_error' in correlation_matrix.columns:\n",
        "    print(f\"\\nüéØ Correlation with Pricing Error:\")\n",
        "    print(\"=\"*50)\n",
        "    target_corr = correlation_matrix['pricing_error'].sort_values(key=abs, ascending=False)\n",
        "    for var, corr in target_corr.items():\n",
        "        if var != 'pricing_error':\n",
        "            print(f\"‚Ä¢ {var}: {corr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Market Microstructure Analysis\n",
        "\n",
        "Analyze bid-ask spreads, volume patterns, and liquidity characteristics of the options.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate bid-ask spread metrics\n",
        "df_analysis['bid_ask_spread'] = df_analysis['ask'] - df_analysis['bid']\n",
        "df_analysis['relative_spread'] = df_analysis['bid_ask_spread'] / df_analysis['market_price']\n",
        "df_analysis['mid_price'] = (df_analysis['bid'] + df_analysis['ask']) / 2\n",
        "\n",
        "print(\"üí∞ Market Microstructure Analysis:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"‚Ä¢ Average bid-ask spread: ${df_analysis['bid_ask_spread'].mean():.4f}\")\n",
        "print(f\"‚Ä¢ Median bid-ask spread: ${df_analysis['bid_ask_spread'].median():.4f}\")\n",
        "print(f\"‚Ä¢ Average relative spread: {df_analysis['relative_spread'].mean():.2%}\")\n",
        "print(f\"‚Ä¢ Median relative spread: {df_analysis['relative_spread'].median():.2%}\")\n",
        "\n",
        "# Volume analysis (handle NaN values)\n",
        "volume_data = df_analysis['volume'].fillna(0)\n",
        "print(f\"\\nüìä Volume Analysis:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"‚Ä¢ Options with volume data: {df_analysis['volume'].notna().sum()}/{len(df_analysis)}\")\n",
        "print(f\"‚Ä¢ Average volume (non-zero): {volume_data[volume_data > 0].mean():.1f}\")\n",
        "print(f\"‚Ä¢ Median volume (non-zero): {volume_data[volume_data > 0].median():.1f}\")\n",
        "print(f\"‚Ä¢ Total volume: {volume_data.sum():.0f}\")\n",
        "\n",
        "print(f\"\\nüè¶ Open Interest Analysis:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"‚Ä¢ Average open interest: {df_analysis['open_interest'].mean():.1f}\")\n",
        "print(f\"‚Ä¢ Median open interest: {df_analysis['open_interest'].median():.1f}\")\n",
        "print(f\"‚Ä¢ Total open interest: {df_analysis['open_interest'].sum():.0f}\")\n",
        "\n",
        "# Create microstructure visualizations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Market Microstructure Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Bid-Ask Spread vs Moneyness\n",
        "axes[0,0].scatter(df_analysis['moneyness'], df_analysis['bid_ask_spread'], alpha=0.6, color='blue')\n",
        "axes[0,0].set_xlabel('Moneyness (S/K)')\n",
        "axes[0,0].set_ylabel('Bid-Ask Spread ($)')\n",
        "axes[0,0].set_title('Bid-Ask Spread vs Moneyness')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Relative Spread vs Moneyness\n",
        "axes[0,1].scatter(df_analysis['moneyness'], df_analysis['relative_spread']*100, alpha=0.6, color='green')\n",
        "axes[0,1].set_xlabel('Moneyness (S/K)')\n",
        "axes[0,1].set_ylabel('Relative Spread (%)')\n",
        "axes[0,1].set_title('Relative Spread vs Moneyness')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Volume vs Open Interest\n",
        "volume_plot = df_analysis['volume'].fillna(0)\n",
        "axes[0,2].scatter(df_analysis['open_interest'], volume_plot, alpha=0.6, color='red')\n",
        "axes[0,2].set_xlabel('Open Interest')\n",
        "axes[0,2].set_ylabel('Volume')\n",
        "axes[0,2].set_title('Volume vs Open Interest')\n",
        "axes[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Spread vs Time to Expiry\n",
        "axes[1,0].scatter(df_analysis['time_to_expiry'], df_analysis['bid_ask_spread'], alpha=0.6, color='purple')\n",
        "axes[1,0].set_xlabel('Time to Expiry (years)')\n",
        "axes[1,0].set_ylabel('Bid-Ask Spread ($)')\n",
        "axes[1,0].set_title('Spread vs Time to Expiry')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Volume Distribution\n",
        "volume_nonzero = volume_plot[volume_plot > 0]\n",
        "if len(volume_nonzero) > 0:\n",
        "    axes[1,1].hist(volume_nonzero, bins=15, alpha=0.7, color='orange', edgecolor='black')\n",
        "    axes[1,1].set_xlabel('Volume')\n",
        "    axes[1,1].set_ylabel('Frequency')\n",
        "    axes[1,1].set_title('Volume Distribution (Non-Zero)')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "else:\n",
        "    axes[1,1].text(0.5, 0.5, 'No Volume Data', ha='center', va='center', transform=axes[1,1].transAxes)\n",
        "    axes[1,1].set_title('Volume Distribution (Non-Zero)')\n",
        "\n",
        "# 6. Open Interest Distribution\n",
        "axes[1,2].hist(df_analysis['open_interest'], bins=15, alpha=0.7, color='cyan', edgecolor='black')\n",
        "axes[1,2].set_xlabel('Open Interest')\n",
        "axes[1,2].set_ylabel('Frequency')\n",
        "axes[1,2].set_title('Open Interest Distribution')\n",
        "axes[1,2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Outlier Detection and Analysis\n",
        "\n",
        "Identify outliers in the data using multiple methods and analyze their impact on the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Outlier detection using multiple methods\n",
        "print(\"üîç Outlier Detection Analysis:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Method 1: Isolation Forest\n",
        "clean_data_iso, outliers_iso = processor.detect_outliers(df_analysis, method='isolation_forest', contamination=0.1)\n",
        "print(f\"‚Ä¢ Isolation Forest: {len(outliers_iso)} outliers ({len(outliers_iso)/len(df_analysis)*100:.1f}%)\")\n",
        "\n",
        "# Method 2: Z-score method\n",
        "clean_data_zscore, outliers_zscore = processor.detect_outliers(df_analysis, method='zscore')\n",
        "print(f\"‚Ä¢ Z-score method: {len(outliers_zscore)} outliers ({len(outliers_zscore)/len(df_analysis)*100:.1f}%)\")\n",
        "\n",
        "# Method 3: IQR method\n",
        "clean_data_iqr, outliers_iqr = processor.detect_outliers(df_analysis, method='iqr')\n",
        "print(f\"‚Ä¢ IQR method: {len(outliers_iqr)} outliers ({len(outliers_iqr)/len(df_analysis)*100:.1f}%)\")\n",
        "\n",
        "# Analyze outliers in pricing errors specifically\n",
        "pricing_error_q1 = df_analysis['pricing_error'].quantile(0.25)\n",
        "pricing_error_q3 = df_analysis['pricing_error'].quantile(0.75)\n",
        "pricing_error_iqr = pricing_error_q3 - pricing_error_q1\n",
        "pricing_error_outliers = df_analysis[\n",
        "    (df_analysis['pricing_error'] < pricing_error_q1 - 1.5 * pricing_error_iqr) |\n",
        "    (df_analysis['pricing_error'] > pricing_error_q3 + 1.5 * pricing_error_iqr)\n",
        "]\n",
        "\n",
        "print(f\"\\nüéØ Pricing Error Outliers:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"‚Ä¢ Pricing error outliers: {len(pricing_error_outliers)} ({len(pricing_error_outliers)/len(df_analysis)*100:.1f}%)\")\n",
        "if len(pricing_error_outliers) > 0:\n",
        "    print(f\"‚Ä¢ Outlier pricing error range: ${pricing_error_outliers['pricing_error'].min():.4f} to ${pricing_error_outliers['pricing_error'].max():.4f}\")\n",
        "    print(f\"‚Ä¢ Mean outlier pricing error: ${pricing_error_outliers['pricing_error'].mean():.4f}\")\n",
        "\n",
        "# Create outlier visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Outlier Detection Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Box plots for key variables\n",
        "key_vars = ['pricing_error', 'relative_error', 'implied_volatility', 'market_price', 'moneyness', 'bid_ask_spread']\n",
        "\n",
        "for i, var in enumerate(key_vars):\n",
        "    row, col = i // 3, i % 3\n",
        "    ax = axes[row, col]\n",
        "    \n",
        "    # Create box plot\n",
        "    data_to_plot = df_analysis[var]\n",
        "    if var == 'relative_error':\n",
        "        data_to_plot = data_to_plot * 100  # Convert to percentage\n",
        "        \n",
        "    box_plot = ax.boxplot(data_to_plot.dropna(), patch_artist=True)\n",
        "    box_plot['boxes'][0].set_facecolor('lightblue')\n",
        "    box_plot['boxes'][0].set_alpha(0.7)\n",
        "    \n",
        "    ax.set_title(f'{var.replace(\"_\", \" \").title()}')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add outlier statistics\n",
        "    q1, q3 = data_to_plot.quantile([0.25, 0.75])\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    outliers = data_to_plot[(data_to_plot < lower_bound) | (data_to_plot > upper_bound)]\n",
        "    \n",
        "    ax.text(0.02, 0.98, f'Outliers: {len(outliers)}\\\\n({len(outliers)/len(data_to_plot)*100:.1f}%)', \n",
        "            transform=ax.transAxes, verticalalignment='top',\n",
        "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed outlier analysis\n",
        "print(f\"\\nüìä Detailed Outlier Analysis:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if len(pricing_error_outliers) > 0:\n",
        "    print(\"Top 5 Pricing Error Outliers:\")\n",
        "    outlier_cols = ['strike', 'market_price', 'bs_price', 'pricing_error', 'moneyness', 'implied_volatility']\n",
        "    print(pricing_error_outliers[outlier_cols].head().to_string())\n",
        "    \n",
        "    print(f\"\\nüîç Outlier Characteristics:\")\n",
        "    print(f\"‚Ä¢ ITM outliers: {(pricing_error_outliers['moneyness'] > 1.0).sum()}\")\n",
        "    print(f\"‚Ä¢ OTM outliers: {(pricing_error_outliers['moneyness'] < 1.0).sum()}\")\n",
        "    print(f\"‚Ä¢ High IV outliers (>50%): {(pricing_error_outliers['implied_volatility'] > 0.5).sum()}\")\n",
        "    print(f\"‚Ä¢ Low time to expiry outliers (<0.01 years): {(pricing_error_outliers['time_to_expiry'] < 0.01).sum()}\")\n",
        "else:\n",
        "    print(\"‚úÖ No significant pricing error outliers found using IQR method\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Key Insights and Summary\n",
        "\n",
        "Summarize the key findings from the exploratory data analysis and their implications for modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive EDA summary\n",
        "print(\"üìã EXPLORATORY DATA ANALYSIS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìä DATASET OVERVIEW:\")\n",
        "print(f\"‚Ä¢ Total options contracts: {len(df_analysis)}\")\n",
        "print(f\"‚Ä¢ Data completeness: {(1 - df_analysis.isnull().sum().sum() / (df_analysis.shape[0] * df_analysis.shape[1]))*100:.1f}%\")\n",
        "print(f\"‚Ä¢ Time range: {df_analysis['time_to_expiry'].min():.4f} to {df_analysis['time_to_expiry'].max():.4f} years\")\n",
        "print(f\"‚Ä¢ Strike range: ${df_analysis['strike'].min():.0f} - ${df_analysis['strike'].max():.0f}\")\n",
        "print(f\"‚Ä¢ Moneyness range: {df_analysis['moneyness'].min():.3f} - {df_analysis['moneyness'].max():.3f}\")\n",
        "\n",
        "print(f\"\\nüéØ PRICING ERROR INSIGHTS:\")\n",
        "mean_error = df_analysis['pricing_error'].mean()\n",
        "std_error = df_analysis['pricing_error'].std()\n",
        "mean_rel_error = df_analysis['relative_error'].mean()\n",
        "print(f\"‚Ä¢ Mean absolute pricing error: ${abs(mean_error):.4f}\")\n",
        "print(f\"‚Ä¢ Pricing error volatility: ${std_error:.4f}\")\n",
        "print(f\"‚Ä¢ Mean relative error: {mean_rel_error:.2%}\")\n",
        "print(f\"‚Ä¢ Error direction: {'Market overpriced' if mean_error > 0 else 'Market underpriced'} vs Black-Scholes\")\n",
        "\n",
        "print(f\"\\nüí∞ MARKET MICROSTRUCTURE:\")\n",
        "print(f\"‚Ä¢ Average bid-ask spread: ${df_analysis['bid_ask_spread'].mean():.4f}\")\n",
        "print(f\"‚Ä¢ Average relative spread: {df_analysis['relative_spread'].mean():.2%}\")\n",
        "print(f\"‚Ä¢ Volume data availability: {df_analysis['volume'].notna().sum()}/{len(df_analysis)} contracts\")\n",
        "print(f\"‚Ä¢ Liquidity concentration: Top 10% OI contracts hold {df_analysis.nlargest(int(len(df_analysis)*0.1), 'open_interest')['open_interest'].sum() / df_analysis['open_interest'].sum() * 100:.1f}% of total OI\")\n",
        "\n",
        "print(f\"\\nüîç DATA QUALITY ASSESSMENT:\")\n",
        "# Count extreme values\n",
        "extreme_iv = (df_analysis['implied_volatility'] > 1.0).sum()\n",
        "extreme_moneyness = ((df_analysis['moneyness'] < 0.5) | (df_analysis['moneyness'] > 2.0)).sum()\n",
        "short_expiry = (df_analysis['time_to_expiry'] < 0.01).sum()\n",
        "\n",
        "print(f\"‚Ä¢ High implied volatility (>100%): {extreme_iv} contracts\")\n",
        "print(f\"‚Ä¢ Extreme moneyness (<0.5 or >2.0): {extreme_moneyness} contracts\")\n",
        "print(f\"‚Ä¢ Very short expiry (<0.01 years): {short_expiry} contracts\")\n",
        "print(f\"‚Ä¢ Outlier rate (Isolation Forest): {len(outliers_iso)/len(df_analysis)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nüìà MODELING IMPLICATIONS:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check for high correlations that might cause multicollinearity\n",
        "high_corr_count = len(high_corr_pairs)\n",
        "print(f\"‚Ä¢ Multicollinearity risk: {high_corr_count} highly correlated pairs found\")\n",
        "\n",
        "# Distribution characteristics\n",
        "skewness_issues = []\n",
        "for var in ['pricing_error', 'relative_error', 'implied_volatility']:\n",
        "    skew_val = stats.skew(df_analysis[var].dropna())\n",
        "    if abs(skew_val) > 1:\n",
        "        skewness_issues.append(f\"{var} (skew: {skew_val:.2f})\")\n",
        "\n",
        "if skewness_issues:\n",
        "    print(f\"‚Ä¢ Distribution concerns: {', '.join(skewness_issues)}\")\n",
        "else:\n",
        "    print(\"‚Ä¢ Distribution quality: Good - no severe skewness detected\")\n",
        "\n",
        "# Feature engineering recommendations\n",
        "print(f\"\\nüõ†Ô∏è RECOMMENDED PREPROCESSING:\")\n",
        "print(\"‚Ä¢ Handle missing volume data (imputation or indicator variables)\")\n",
        "print(\"‚Ä¢ Consider log transformation for skewed variables\")\n",
        "if high_corr_count > 0:\n",
        "    print(\"‚Ä¢ Apply dimensionality reduction or feature selection for correlated variables\")\n",
        "print(\"‚Ä¢ Robust scaling recommended due to outliers\")\n",
        "print(\"‚Ä¢ Consider separate models for different moneyness regimes\")\n",
        "\n",
        "print(f\"\\n‚úÖ DATASET READINESS:\")\n",
        "data_quality_score = (\n",
        "    (1 - df_analysis.isnull().sum().sum() / (df_analysis.shape[0] * df_analysis.shape[1])) * 30 +  # Completeness\n",
        "    (1 - len(outliers_iso)/len(df_analysis)) * 30 +  # Outlier rate\n",
        "    (1 - min(abs(mean_rel_error), 0.5) / 0.5) * 20 +  # Pricing error magnitude\n",
        "    (1 - min(high_corr_count, 10) / 10) * 20  # Multicollinearity\n",
        ") \n",
        "print(f\"Overall data quality score: {data_quality_score:.1f}/100\")\n",
        "\n",
        "if data_quality_score >= 80:\n",
        "    print(\"üü¢ EXCELLENT - Dataset ready for modeling\")\n",
        "elif data_quality_score >= 60:\n",
        "    print(\"üü° GOOD - Minor preprocessing needed\")\n",
        "else:\n",
        "    print(\"üî¥ NEEDS WORK - Significant data issues to address\")\n",
        "\n",
        "print(f\"\\nüìÅ Analysis complete! Processed data available at:\")\n",
        "print(f\"   {latest_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sys.path.append('..')\n",
        "\n",
        "from config import Config\n",
        "from src.black_scholes import BlackScholesCalculator\n",
        "from src.data_processor import OptionsDataProcessor\n",
        "\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"Ready for EDA\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
